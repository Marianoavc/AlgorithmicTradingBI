# -*- coding: utf-8 -*-
"""Eq.D_Buenaventura

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TwldTMHFwQpVQarDqC2pmdoZsCe54Vvr

# IMPORTING LIBRARIES
"""

# Basicos
import pandas as pd # Manipulación y análisis de datos
import numpy as np # Operaciones numéricas y algebra lineal

# Extraer información del instrumento financiero
import yfinance as yf # Descarga de datos financieros desde Yahoo Finance

# Preprocesamiento
from sklearn.model_selection import train_test_split # División de los datos en conjuntos de entrenamiento y prueba
from sklearn.preprocessing import StandardScaler # Escalado de características
from sklearn.preprocessing import MinMaxScaler # Escalado de características a un rango específico
from statsmodels.tsa.seasonal import seasonal_decompose # Descomposición de series temporales

# Evaluacion
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error # Métricas de evaluación

# Visualizaciones
import matplotlib.pyplot as plt # Generación de gráficos y visualizaciones
import seaborn as sns # Visualización de datos estadísticos

# RNN: LSTM
from tensorflow.keras.models import Sequential # Creación de modelos secuenciales de Keras
from tensorflow.keras.layers import LSTM, Dense # Capas LSTM y densas para redes neuronales

# Support Vector Regressor
from sklearn.model_selection import GridSearchCV # Búsqueda en cuadrícula para optimización de hiperparámetros
from sklearn.svm import SVR # Soporte vectorial para regresión
from sklearn.pipeline import Pipeline # Creación de pipelines para flujos de trabajo de ML

# ANN: MLP Regressor
from sklearn.neural_network import MLPRegressor # Regressor de perceptrón multicapa

# Modelo Híbrido
from sklearn.kernel_approximation import RBFSampler # Aproximación de kernel de base radial (RBF)
from sklearn.linear_model import Ridge # Regresión Ridge
from sklearn.pipeline import make_pipeline # Creación de pipelines para flujos de trabajo de ML

"""# COMPAÑIA DE MINAS BUENAVENURA SAA (BVN)

## Extracción de datos
"""

# Descargar datos históricos de Buenaventura en un rango de fechas específico
bvn_df = yf.download('BVN', start='2018-01-01', end='2024-01-01')

"""## Análisis Exploratorio de datos (EDA)

Información del dataframe
"""

# Mostrar las primeras filas del DataFrame
print(bvn_df.head())

bvn_df.info()

bvn_df.describe()

# Calcular la media móvil exponencial
bvn_df['EMA'] = bvn_df['Open'].ewm(span=20, adjust=False).mean()

# Graficar la variación en el tiempo del precio de apertura y la media móvil exponencial
plt.figure(figsize=(12, 6))
plt.plot(bvn_df['Open'], label='Open Price')
plt.plot(bvn_df['EMA'], label='EMA (20 días)', color='red')
plt.title('Variación en el tiempo del precio de apertura (Open) con EMA')
plt.xlabel('Fecha')
plt.ylabel('Precio de apertura')
plt.legend()
plt.grid(True)
plt.gca().spines[['top', 'right']].set_visible(False)

# Mostrar la gráfica
plt.show()

"""Aumento y Caída (2018-2020): Vemos que desde 2018 hasta principios de 2020, hubo una tendencia general al alza en el precio de apertura, alcanzando un pico a principios de 2020.

Caída Significativa (2020): A principios de 2020, hay una caída abrupta y significativa en el precio de apertura, lo cual podría estar relacionado con la crisis del COVID-19 que afectó a los mercados globales.

Recuperación y Fluctuaciones (2020-2021): Tras la caída, el precio comenzó a recuperarse, mostrando varias fluctuaciones a lo largo del tiempo, pero sin alcanzar los niveles pre-crisis.
"""

# Variacion del volumen (Volume) en el tiempo (escala = e8)
sns.lineplot(data=bvn_df, x="Date", y="Volume")
plt.title("Volume Over Time")
plt.show()

# Calcular los retornos diarios
bvn_df['Returns'] = bvn_df['Close'].pct_change()

# Graficar la distribución de los retornos
plt.figure(figsize=(10, 6))
sns.histplot(bvn_df['Returns'].dropna(), kde=True, bins=50, color='blue')
plt.title('Distribución de Retornos Diarios')
plt.xlabel('Retornos')
plt.ylabel('Frecuencia')
plt.grid(True)
plt.gca().spines[['top', 'right']].set_visible(False)
plt.show()

"""El pico más alto de la distribución está cerca de cero, indicando que la mayoría de los retornos diarios son pequeños (cercanos al 0%).
Esto sugiere que en promedio, las variaciones diarias en el precio de la acción son pequeñas, con retornos diarios promedio cercanos a cero.
"""

# Descomposición de la serie temporal
decomposition = seasonal_decompose(bvn_df['Close'].dropna(), model='multiplicative', period=365)
decomposition.plot()
plt.show()

"""Serie Temporal Original (Close)
Gráfico de la serie temporal original que muestra el precio de cierre diario a lo largo del tiempo.

Componente de Tendencia (Trend)
Gráfico de la tendencia que muestra la dirección general del precio de cierre, eliminando las fluctuaciones a corto plazo.

Componente Estacional (Seasonal)
Gráfico estacional que muestra los patrones que se repiten anualmente en la serie temporal.

Componente de Residuos (Resid)
Gráfico de residuos que muestra las variaciones no explicadas por la tendencia ni por los patrones estacionales.

##PREPROCESAMIENTO

Como el dataframe no contiene nulos procedemos a la creación de nuevas variables
"""

# Añadir nuevas columnas basadas en la descripción
bvn_df['Precio Anterior'] = bvn_df['Close'].shift(1)
bvn_df['Precio Máximo Anterior'] = bvn_df['High'].shift(1)
bvn_df['Precio Mínimo Anterior'] = bvn_df['Low'].shift(1)
bvn_df['Precio Apertura Anterior'] = bvn_df['Open'].shift(1)
bvn_df['PM_10'] = bvn_df['Close'].rolling(window=10).mean()

# Calcular bandas de Bollinger
bvn_df['Middle Band Bollinger'] = bvn_df['Close'].rolling(window=20).mean()
bvn_df['Upper Band Bollinger'] = bvn_df['Middle Band Bollinger'] + 1.96 * bvn_df['Close'].rolling(window=20).std()
bvn_df['Lower Band Bollinger'] = bvn_df['Middle Band Bollinger'] - 1.96 * bvn_df['Close'].rolling(window=20).std()

# Gráfico: Precio de Cierre con Bandas de Bollinger
plt.figure(figsize=(12, 6))
plt.plot(bvn_df.index, bvn_df['Close'], label='Precio de Cierre')
plt.plot(bvn_df.index, bvn_df['Upper Band Bollinger'], label='Banda Superior Bollinger', color='green')
plt.plot(bvn_df.index, bvn_df['Lower Band Bollinger'], label='Banda Inferior Bollinger', color='red')
plt.fill_between(bvn_df.index, bvn_df['Upper Band Bollinger'], bvn_df['Lower Band Bollinger'], color='gray', alpha=0.1)
plt.title('Precio de Cierre con Bandas de Bollinger')
plt.xlabel('Fecha')
plt.ylabel('Precio')
plt.legend()
plt.grid(True)
plt.show()

# Calcular precio medio
bvn_df['Precio Medio'] = (bvn_df['High'] + bvn_df['Low'] + bvn_df['Close']) / 3

# Precio de la plata
silver_df = yf.download('SI=F', start='2018-01-01', end='2024-01-01')
bvn_df['Precio Plata'] = silver_df['Close']

# Gráfico: Comparación del Precio de Cierre con el Precio de la Plata(onza troy = 31,1 gramos)
plt.figure(figsize=(12, 6))
plt.plot(bvn_df.index, bvn_df['Close'], label='Precio de Cierre Buenaventura')
plt.plot(bvn_df.index, bvn_df['Precio Plata'], label='Precio de la Plata', color='silver')
plt.title('Comparación del Precio de Cierre con el Precio de la Plata')
plt.xlabel('Fecha')
plt.ylabel('Precio')
plt.legend()
plt.grid(True)
plt.show()

# Eliminar filas con valores NaN generados por los cálculos de rolling y shift
bvn_df.dropna(inplace=True)

# Mostrar las primeras filas del DataFrame para verificar las nuevas columnas
print(bvn_df.head())

"""## MODELO: Red Neuronal Recurrente Long Short Term Memory (LSTM)

Modelado de X e y
"""

# Seleccionar las columnas que quieres usar como input para el modelo LSTM
features = bvn_df[['Open', 'High', 'Low', 'Close', 'Volume', 'Precio Anterior',
                   'Precio Máximo Anterior', 'Precio Mínimo Anterior', 'Precio Apertura Anterior',
                   'PM_10', 'Middle Band Bollinger', 'Upper Band Bollinger',
                   'Lower Band Bollinger', 'Precio Medio', 'Precio Plata']].values

# Escalar los datos
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_features = scaler.fit_transform(features)

# Definir la longitud de la secuencia (número de timesteps)
timesteps = 60

# Prepara los datos para la LSTM
X = []
y = []

for i in range(timesteps, len(scaled_features)):
    X.append(scaled_features[i-timesteps:i])
    y.append(scaled_features[i, 3])  # Prediccion del precio de cierre (Close)

X, y = np.array(X), np.array(y)

"""Separación en Conjunto de Entrenamiento y Prueba

"""

# Divide en conjuntos de entrenamiento y prueba
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

"""### Definición y Compilación del Modelo LSTM"""

# Definición y Compilación del Modelo LSTM

# Construir el modelo LSTM
model = Sequential() # Se inicializa un modelo secuencial

# Se añade una capa LSTM con 50 unidades, que devuelve secuencias para ser utilizadas en la siguiente capa LSTM
# Se especifica la forma de entrada como (número de timesteps, número de características)
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))

# Se añade una segunda capa LSTM con 50 unidades, sin devolver secuencias (última capa LSTM)
model.add(LSTM(units=50))

# Se añade una capa densa completamente conectada con una unidad de salida (predicción final)
model.add(Dense(1))

# Compilar el modelo
# Se especifica el optimizador 'adam' y la función de pérdida 'mean_squared_error'
model.compile(optimizer='adam', loss='mean_squared_error')

# Entrenar el modelo
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

"""Predicciones"""

# Hacer predicciones
predictions = model.predict(X_test)

# Invertir la escala de las predicciones
predictions = scaler.inverse_transform(np.concatenate((predictions, np.zeros((predictions.shape[0], scaled_features.shape[1]-1))), axis=1))[:,0]

"""Gráfica de las Predicciones y Valores Reales"""

# Crear un DataFrame para las predicciones y los valores reales
predictions_df = pd.DataFrame({
    'Fecha': bvn_df.index[-len(predictions):],
    'Predicciones': predictions,
    'Valores Reales': scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], scaled_features.shape[1]-1))), axis=1))[:,0]
})

# Graficar las predicciones y los valores reales
plt.figure(figsize=(14, 7))
plt.plot(predictions_df['Fecha'], predictions_df['Valores Reales'], label='Valores Reales')
plt.plot(predictions_df['Fecha'], predictions_df['Predicciones'], label='Predicciones', color='brown')
plt.title('Predicciones vs Valores Reales del Precio de Cierre')
plt.xlabel('Fecha')
plt.ylabel('Precio de Cierre')
plt.legend()
plt.grid(True)
plt.show()

"""### Evaluación del modelo"""

# Calcular las métricas de evaluación
mse = mean_squared_error(predictions_df['Valores Reales'], predictions_df['Predicciones'])
rmse = np.sqrt(mse)
mape = mean_absolute_percentage_error(predictions_df['Valores Reales'], predictions_df['Predicciones'])

# Imprimir las métricas con nombres completos
print(f'Error Cuadrático Medio (MSE): {mse}')
print(f'Raíz del Error Cuadrático Medio (RMSE): {rmse}')
print(f'Error Absoluto Porcentual Promedio (MAPE): {mape}')

"""## MODELO: Support Vector Regressor"""

# Seleccionar las columnas que quieres usar como input para el modelo LSTM
features = [ 'Precio Anterior', 'Precio Máximo Anterior', 'Precio Mínimo Anterior', 'Precio Apertura Anterior',
                   'PM_10', 'Middle Band Bollinger', 'Upper Band Bollinger',
                   'Lower Band Bollinger', 'Precio Plata']

X=bvn_df[features]
y=bvn_df['Close']

# División de los datos en conjuntos de entrenamiento y prueba
split = int(0.8 * len(bvn_df))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

"""### Definición de los parámetros del modelo y predicción"""

# Definición de los parámetros para GridSearchCV
param_grid = {
    'svr__kernel': ['poly'],
    'svr__degree': [1, 2],
    'svr__C': [2**-1, 1, 2, 3, 10, 20, 100]
}

# Crear un pipeline que incluya la estandarización y el modelo SVR
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svr', SVR())
])

# Configurar GridSearchCV
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', error_score='raise')

# Entrenar el modelo
grid_search.fit(X_train, y_train)

# Obtener los mejores parámetros
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Mostrar los mejores parámetros
print("Mejores parámetros:", best_params)

# Realizar predicciones con el conjunto de prueba
y_pred = best_model.predict(X_test)

# Asumimos que ya tienes y_test y y_pred de las celdas anteriores

# Crear un DataFrame con las fechas y los valores de prueba y predicciones
result_df = pd.DataFrame({'Fecha': X_test.index, 'Valores Reales': y_test, 'Predicciones': y_pred})

# Graficar las predicciones vs los valores reales
plt.figure(figsize=(14, 7))
plt.plot(result_df['Fecha'], result_df['Valores Reales'], label='Valores Reales')
plt.plot(result_df['Fecha'], result_df['Predicciones'], label='Predicciones', color='red', linestyle='--')
plt.title('Comparación de Predicciones y Valores Reales')
plt.xlabel('Fecha')
plt.ylabel('Precio de Cierre')
plt.legend()
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""### Evaluación del modelo"""

# Calcular métricas de evaluación
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mape = np.mean((np.abs(y_test - y_pred) / y_test)) * 100

# Mostrar las métricas con nombres completos
print(f'Error Cuadrático Medio (MSE): {mse}')
print(f'Raíz del Error Cuadrático Medio (RMSE): {rmse}')
print(f'Error Absoluto Porcentual Promedio (MAPE): {mape}')

# Calcular los errores
errors = result_df['Valores Reales'] - result_df['Predicciones']

# Graficar la distribución de los errores
plt.figure(figsize=(10, 6))
sns.histplot(errors, kde=True, bins=30, color='blue')
plt.title('Distribución de los Errores')
plt.xlabel('Error')
plt.ylabel('Frecuencia')
plt.grid(True)
plt.show()

"""## MODELO: Red Neuronal Artificial (MLP Regressor)

"""

# Seleccionar las columnas que quieres usar como input para el modelo
features = bvn_df[['Open', 'High', 'Low', 'Close', 'Volume', 'Precio Anterior',
                   'Precio Máximo Anterior', 'Precio Mínimo Anterior', 'Precio Apertura Anterior',
                   'PM_10', 'Middle Band Bollinger', 'Upper Band Bollinger',
                   'Lower Band Bollinger', 'Precio Medio', 'Precio Plata']].values

# Escalar los datos
# Se instancia el escalador MinMaxScaler para escalar las características entre 0 y 1
scaler = MinMaxScaler(feature_range=(0, 1))
# Se ajusta el escalador a los datos y se transforman las características
scaled_features = scaler.fit_transform(features)

# Definir la longitud de la secuencia (número de timesteps)
# Se define la cantidad de pasos temporales que se considerarán para cada muestra
timesteps = 60

# Preparar los datos para el modelo
# Inicialización de las listas para almacenar las secuencias de entrada y los valores de salida
X = []
y = []

# Crear secuencias de datos para entrenar el modelo
# Se itera sobre el rango de datos, creando secuencias de 'timesteps' longitud
for i in range(timesteps, len(scaled_features)):
    # Se añaden los 'timesteps' datos anteriores a la lista de entradas X
    X.append(scaled_features[i-timesteps:i])
    # Se añade el precio de cierre actual (índice 3) a la lista de salidas y
    y.append(scaled_features[i, 3])

# Convertir las listas a arrays numpy para que puedan ser utilizados por los modelos de ML
X, y = np.array(X), np.array(y)

# Divide en conjuntos de entrenamiento y prueba
# Se define el índice de división para el 80% de los datos como conjunto de entrenamiento
split = int(0.8 * len(X))
# Se dividen los datos en conjuntos de entrenamiento y prueba
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

"""### Definicion del modelo"""

# Definir y entrenar el modelo MLP
model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=1)
model_mlp.fit(X_train.reshape(X_train.shape[0], -1), y_train)

# Hacer predicciones
predictions_mlp = model_mlp.predict(X_test.reshape(X_test.shape[0], -1))

# Invertir la escala de las predicciones y de y_test
predictions_mlp = scaler.inverse_transform(np.concatenate((predictions_mlp.reshape(-1, 1), np.zeros((predictions_mlp.shape[0], scaled_features.shape[1]-1))), axis=1))[:,0]
y_test_mlp = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], scaled_features.shape[1]-1))), axis=1))[:,0]

# Asegurarse de que test_dates, predictions_mlp y y_test_mlp tengan la misma longitud
test_dates = bvn_df.index[-len(predictions_mlp):]

# Crear un DataFrame para las predicciones y los valores reales en el conjunto de prueba
predictions_df_mlp = pd.DataFrame({
    'Fecha': test_dates,
    'Predicciones MLP': predictions_mlp,
    'Valores Reales': y_test_mlp
})

# Graficar las predicciones y los valores reales del conjunto de prueba para MLP
plt.figure(figsize=(14, 7))
plt.plot(predictions_df_mlp['Fecha'], predictions_df_mlp['Valores Reales'], label='Valores Reales')
plt.plot(predictions_df_mlp['Fecha'], predictions_df_mlp['Predicciones MLP'], label='Predicciones MLP', color='blue')
plt.title('Predicciones MLP vs Valores Reales del Precio de Cierre (Conjunto de Prueba)')
plt.xlabel('Fecha')
plt.ylabel('Precio de Cierre')
plt.legend()
plt.grid(True)
plt.show()

"""### Evaluación del modelo"""

# Calcular las métricas de evaluación
mse_mlp = mean_squared_error(y_test_mlp, predictions_mlp)
rmse_mlp = np.sqrt(mse_mlp)
mape_mlp = mean_absolute_percentage_error(y_test_mlp, predictions_mlp)

# Imprimir las métricas con nombres completos
print(f'Error Cuadrático Medio (MSE) para MLP: {mse_mlp}')
print(f'Raíz del Error Cuadrático Medio (RMSE) para MLP: {rmse_mlp}')
print(f'Error Absoluto Porcentual Promedio (MAPE) para MLP: {mape_mlp}')

"""## MODELO: MODELO HÍBRIDO (RBF+SVR)

"""

# Seleccionar las columnas que quieres usar como input para el modelo
features = bvn_df[['Open', 'High', 'Low', 'Close', 'Volume', 'Precio Anterior',
                   'Precio Máximo Anterior', 'Precio Mínimo Anterior', 'Precio Apertura Anterior',
                   'PM_10', 'Middle Band Bollinger', 'Upper Band Bollinger',
                   'Lower Band Bollinger', 'Precio Medio', 'Precio Plata']].values

# Escalar los datos
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_features = scaler.fit_transform(features)

# Definir la longitud de la secuencia (número de timesteps)
timesteps = 60

# Prepara los datos para el modelo
X = []
y = []

for i in range(timesteps, len(scaled_features)):
    X.append(scaled_features[i-timesteps:i])
    y.append(scaled_features[i, 3])  # Supongamos que quieres predecir el precio de cierre (Close)

X, y = np.array(X), np.array(y)

# Divide en conjuntos de entrenamiento y prueba
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

"""### Definicion del modelo"""

# Ajustar gamma y aumentar el número de componentes
# Se crea un transformador RBF Sampler con un valor específico de gamma y un número específico de componentes
rbf_feature = RBFSampler(gamma=0.01, n_components=500, random_state=1)
# Se crea un modelo de Support Vector Regression (SVR) con kernel lineal
svm = SVR(kernel='linear')
# Se combina el RBF Sampler y el SVR en un pipeline para formar el modelo híbrido
model_rbf = make_pipeline(rbf_feature, svm)

# Entrenar el modelo
# Se ajusta el pipeline a los datos de entrenamiento, reestructurando X_train para que tenga el formato adecuado
model_rbf.fit(X_train.reshape(X_train.shape[0], -1), y_train)

# Hacer predicciones
# Se realizan predicciones sobre el conjunto de prueba, reestructurando X_test para que tenga el formato adecuado
predictions_rbf = model_rbf.predict(X_test.reshape(X_test.shape[0], -1))

# Invertir la escala de las predicciones y de y_test
# Se concatenan las predicciones con ceros para ajustarlas a la estructura original antes del escalado y se invierte la escala
predictions_rbf = scaler.inverse_transform(
    np.concatenate((predictions_rbf.reshape(-1, 1),
                    np.zeros((predictions_rbf.shape[0], scaled_features.shape[1] - 1))), axis=1))[:, 0]
# Se hace lo mismo para y_test
y_test_rbf = scaler.inverse_transform(
    np.concatenate((y_test.reshape(-1, 1),
                    np.zeros((y_test.shape[0], scaled_features.shape[1] - 1))), axis=1))[:, 0]

# Crear un DataFrame para las predicciones y los valores reales en el conjunto de prueba
test_dates = bvn_df.index[-len(predictions_rbf):]
predictions_df_rbf = pd.DataFrame({
    'Fecha': test_dates,
    'Predicciones RBF': predictions_rbf,
    'Valores Reales': y_test_rbf
})

# Graficar las predicciones y los valores reales del conjunto de prueba para RBF
plt.figure(figsize=(14, 7))
plt.plot(predictions_df_rbf['Fecha'], predictions_df_rbf['Valores Reales'], label='Valores Reales')
plt.plot(predictions_df_rbf['Fecha'], predictions_df_rbf['Predicciones RBF'], label='Predicciones RBF', color='red')
plt.title('Predicciones RBF vs Valores Reales del Precio de Cierre (Conjunto de Prueba)')
plt.xlabel('Fecha')
plt.ylabel('Precio de Cierre')
plt.legend()
plt.grid(True)
plt.show()

"""### Evaluación del modelo"""

# Calcular las métricas de evaluación
mse_rbf = mean_squared_error(y_test_rbf, predictions_rbf)
rmse_rbf = np.sqrt(mse_rbf)
mape_rbf = mean_absolute_percentage_error(y_test_rbf, predictions_rbf)

# Imprimir las métricas con nombres completos
print(f'Error Cuadrático Medio (MSE) para RBF: {mse_rbf}')
print(f'Raíz del Error Cuadrático Medio (RMSE) para RBF: {rmse_rbf}')
print(f'Error Absoluto Porcentual Promedio (MAPE) para RBF: {mape_rbf}')